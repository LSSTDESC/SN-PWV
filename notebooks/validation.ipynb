{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e8f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from scipy.stats.stats import pearsonr\n",
    "from egon.pipeline import Pipeline\n",
    "from egon.mock import MockTarget\n",
    "from typing import *\n",
    "from snat_sim.types import *\n",
    "from snat_sim.plasticc import PLAsTICC\n",
    "from snat_sim.pipeline.nodes import *\n",
    "\n",
    "from snat_sim.pipeline import FittingPipeline\n",
    "from snat_sim.models import SNModel, ReferenceCatalog, StaticPWVTrans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a712f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: AstropyDeprecationWarning: The update_default_config function is deprecated and may be removed in a future version. [sncosmo]\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "\n",
    "from egon.pipeline import Pipeline\n",
    "from egon.mock import MockTarget\n",
    "\n",
    "from snat_sim.types import *\n",
    "from snat_sim.plasticc import PLAsTICC\n",
    "from snat_sim.pipeline.nodes import *\n",
    "from snat_sim.models import SNModel, ReferenceCatalog, StaticPWVTrans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f400b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    cadence: str,\n",
    "    sim_model: SNModel,\n",
    "    fit_model: SNModel,\n",
    "    vparams: List[str],\n",
    "    out_path: Union[str, Path],\n",
    "    fitting_pool: int = 1,\n",
    "    simulation_pool: int = 1,\n",
    "    writing_pool: int = 1,\n",
    "    bounds: Dict[str, Tuple[float, float]] = None,\n",
    "    max_queue: int = 100,\n",
    "    iter_lim: int = float('inf'),\n",
    "    catalog = None,\n",
    "    add_scatter: bool = True,\n",
    "    fixed_snr: Optional[float] = None,\n",
    "    overwrite: bool = False,\n",
    "    write_lc_sims: bool = False\n",
    ") -> None:\n",
    "    \"\"\"Fit light-curves using multiple processes and combine results into an output file\n",
    "\n",
    "    Args:\n",
    "        cadence: Cadence to use when simulating light-curves\n",
    "        sim_model: Model to use when simulating light-curves\n",
    "        fit_model: Model to use when fitting light-curves\n",
    "        vparams: List of parameter names to vary in the fit\n",
    "        out_path: Path to write results to\n",
    "        fitting_pool: Number of child processes allocated to simulating light-curves\n",
    "        simulation_pool: Number of child processes allocated to fitting light-curves\n",
    "        bounds: Bounds to impose on ``fit_model`` parameters when fitting light-curves\n",
    "        max_queue: Maximum number of light-curves to store in pipeline at once\n",
    "        iter_lim: Limit number of processed light-curves (Useful for profiling)\n",
    "        catalog: Reference star catalog to calibrate simulated supernova with\n",
    "        add_scatter: Add randomly generated scatter to simulated light-curve points\n",
    "        fixed_snr: Simulate light-curves with a fixed signal to noise ratio\n",
    "        overwrite: Whether to allow overwriting an existing output file\n",
    "        write_lc_sims: Include simulated light_curves in the\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the nodes of the analysis pipeline\n",
    "    cadence = PLAsTICC(cadence, model=11)\n",
    "    load_plastic = LoadPlasticcCadence(cadence, iter_lim=iter_lim)\n",
    "    write_to_disk = MockTarget()\n",
    "\n",
    "    simulate_light_curves = SimulateLightCurves(\n",
    "        sn_model=sim_model,\n",
    "        catalog=catalog,\n",
    "        num_processes=simulation_pool,\n",
    "        add_scatter=add_scatter,\n",
    "        fixed_snr=fixed_snr\n",
    "    )\n",
    "\n",
    "    fit_light_curves = FitLightCurves(\n",
    "        sn_model=fit_model,\n",
    "        vparams=vparams,\n",
    "        bounds=bounds,\n",
    "        num_processes=fitting_pool)\n",
    "\n",
    "    # Connect pipeline nodes together\n",
    "    load_plastic.output.connect(simulate_light_curves.input)\n",
    "    simulate_light_curves.success_output.connect(fit_light_curves.input)\n",
    "    simulate_light_curves.failure_output.connect(write_to_disk.input)\n",
    "    fit_light_curves.success_output.connect(write_to_disk.input)\n",
    "    fit_light_curves.failure_output.connect(write_to_disk.input)\n",
    "    \n",
    "    load_plastic.execute()\n",
    "    simulate_light_curves.execute()\n",
    "    fit_light_curves.execute()\n",
    "    write_to_disk.execute()\n",
    "    return write_to_disk.accumulated_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e32c3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alt_sched_rolling: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Model for the atmospheric tranmission\n",
    "transmission_effect = StaticPWVTrans()\n",
    "transmission_effect.set(pwv=4)\n",
    "\n",
    "# Models for fitting/simulating SNE\n",
    "simulation_model = SNModel('salt2-extended')\n",
    "fitting_model = SNModel('salt2-extended')\n",
    "for model in (simulation_model, fitting_model):\n",
    "    model.add_effect(transmission_effect, 'pwv', 'obs')\n",
    "\n",
    "# Model for the reference stellar catalag\n",
    "stellar_catalog = ReferenceCatalog('G2', 'M5', 'K2')\n",
    "\n",
    "data = validate(\n",
    "    cadence='alt_sched_rolling',\n",
    "    sim_model=simulation_model,\n",
    "    fit_model=fitting_model,\n",
    "    catalog=stellar_catalog,\n",
    "    out_path='temp.h5',\n",
    "    vparams=('x0', 'x1', 'c')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36edd898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_data_table(directory, key):\n",
    "    \"\"\"Return a single data table from directory of HDF5 files\n",
    "    \n",
    "    Returns concatenated tables from each of the files.\n",
    "    \n",
    "    Args:\n",
    "        directory (Path): The directory to parse data files from\n",
    "        key        (str): Key of the table in the HDF5 files\n",
    "        \n",
    "    Returns:\n",
    "        A pandas datafram with pipeline data\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if not (h5_files := list(directory.glob('*.h5'))):\n",
    "        raise ValueError(f'No h5 files found in {directory}')\n",
    "    \n",
    "    dataframes = []    \n",
    "    for file in h5_files:\n",
    "        print(file)\n",
    "        with pd.HDFStore(file, 'r') as datastore:\n",
    "            dataframes.append(datastore.get(key))\n",
    "            \n",
    "    return pd.concat(dataframes).set_index('snid')\n",
    "\n",
    "\n",
    "def load_pipeline_data(directory):\n",
    "    \"\"\"Return the combined input and output parameters from a pipeline run\n",
    "    \n",
    "    Args:\n",
    "        directory          (Path): The directory to parse data files from\n",
    "        \n",
    "    Returns:\n",
    "        A pandas datafram with pipeline data\n",
    "    \"\"\"\n",
    "    \n",
    "    sim_params = get_combined_data_table(directory, '/simulation/params')\n",
    "    fit_results = get_combined_data_table(directory, '/fitting/params')\n",
    "\n",
    "    # Combine the imput simulation parameters and the fit results into a single dataframe\n",
    "    # The keys in ``fit_results`` are expected to be a proper subset of ``sim_params``\n",
    "    # so we left join on ``fit_results``\n",
    "    pipeline_data = fit_results.join(sim_params)\n",
    "\n",
    "    # Join results for failed fit results will be nan.\n",
    "    return pipeline_data.replace(-99.99, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11628eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('.').resolve()\n",
    "load_pipeline_data(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.HDFStore('/home/djperrefort/Github/SN-PWV/notebooks/.temp_fn0.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c7fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73132169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SN-PWV] *",
   "language": "python",
   "name": "conda-env-SN-PWV-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
